{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Improving Cat Classification with CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEWfO_Ko0gwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Activation\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRres7UR0gwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRtCyQTD0gwv",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Overview of the data set\n",
        "\n",
        "First of all, we'll load the data. After loading, here's a basic overview;\n",
        "\n",
        "- a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
        "- a test set of m_test images labeled as cat or non-cat\n",
        "- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpA7D37p0gww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = h5py.File('./train.h5', \"r\")\n",
        "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
        "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
        "\n",
        "test_dataset = h5py.File('./test.h5', \"r\")\n",
        "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
        "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydktobJO0gwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6e-Rsiv0gw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "print(train_set_y.shape)\n",
        "print(test_set_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAKDeHHr0gw3",
        "colab_type": "text"
      },
      "source": [
        "Each line of train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boxbCxcM0gw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of a picture\n",
        "index = 60\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN1zPcS60gw6",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Data preprocessing\n",
        "\n",
        "Following are the steps to preprocess data for a ConvNet.\n",
        "\n",
        "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
        "- \"Standardize\" the data\n",
        "- Data Augmentation (will discuss later in the course)\n",
        "\n",
        "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO37NDWu0gw7",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Dimensions of our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PX9K5j00gw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_train = train_set_x_orig.shape[0]\n",
        "m_test = test_set_x_orig.shape[0]\n",
        "num_px = train_set_x_orig.shape[1]\n",
        "\n",
        "print (\"Dataset dimensions:\")\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnTPZzqB0gw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_y = train_set_y.T\n",
        "test_set_y = test_set_y.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvJsbeSJ0gxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1txxC9u0gxD",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Standardizing data\n",
        "\n",
        "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
        "\n",
        "Let's standardize our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kskVt640gxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_x = train_set_x_orig/255.\n",
        "test_set_x = test_set_x_orig/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYqo9MPj0gxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLiCzqr70gxI",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zL1xIlx0gxJ",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Utility function to train & evaluate our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hz83Xdz0gxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function\n",
        "def evaluate_this_model(model, epochs):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    history = model.fit(train_set_x, train_set_y, epochs=epochs)\n",
        "    results = model.evaluate(test_set_x, test_set_y)\n",
        "    \n",
        "    plt.plot(np.squeeze(history.history[\"loss\"]))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\\nAccuracy on training set is {}\".format(history.history[\"acc\"][-1]))\n",
        "    print(\"\\nAccuracy on test set is {}\".format(results[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPS-RFN20gxL",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HupQAAj30gxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(64,64,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(20, padding=\"same\", kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=35, activation='relu'))\n",
        "model.add(Dense(units=15, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZhs4Dk0gxO",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD79_Z4i0gxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0001\n",
        "opt = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvIFagPa0gxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MRRVO05_0gxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_this_model(model, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIm3BRr90gxV",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Improving Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXb1CS090gxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_my_model(epochs):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    for i in range(5):\n",
        "        model = get_model()\n",
        "        history = model.fit(train_set_x, train_set_y, epochs=epochs)\n",
        "        results = model.evaluate(test_set_x, test_set_y)\n",
        "        train_accuracies.append(history.history[\"acc\"][-1])\n",
        "        test_accuracies.append(results[1])\n",
        "\n",
        "    plt.plot(np.squeeze(history.history[\"loss\"]))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Last iteration\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(train_accuracies)\n",
        "    print(test_accuracies)\n",
        "    print(\"\\n\\nAccuracy on training set is {}\".format(np.mean(train_accuracies)))\n",
        "    print(\"\\nAccuracy on test set is {}\".format(np.mean(test_accuracies)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzrmOvR50gxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    \n",
        "    learning_rate = 0.0001\n",
        "    opt = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AQ39l0a0gxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "evaluate_my_model(epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}